public class TextKafkaScheme extends KafkaScheme<String, String, TextKafkaScheme.Context, TextKafkaScheme.Context> { public static final Fields TOPIC_FIELDS = new Fields( "topic", String.class ); public static final Fields PARTITION_FIELDS = new Fields( "partition", int.class ); public static final Fields OFFSET_FIELDS = new Fields( "offset", long.class ); public static final Fields KEY_FIELDS = new Fields( "key", String.class ); public static final Fields VALUE_FIELDS = new Fields( "value", String.class ); public static final Fields TIMESTAMP_FIELDS = new Fields( "timestamp", long.class ); public static final Fields TIMESTAMP_TYPE_FIELDS = new Fields( "timestampType", String.class ); public static final Fields DEFAULT_SOURCE_FIELDS = TOPIC_FIELDS.append( PARTITION_FIELDS ).append( OFFSET_FIELDS ).append( KEY_FIELDS ).append( VALUE_FIELDS ).append( TIMESTAMP_FIELDS ).append( TIMESTAMP_TYPE_FIELDS ); class Context { String[] topics; public Context( String[] topics ) { this.topics = topics; } } public TextKafkaScheme() { super( DEFAULT_SOURCE_FIELDS ); } public TextKafkaScheme( Fields sourceFields ) { super( sourceFields ); if( sourceFields.size() != 7 ) throw new IllegalArgumentException( "wrong number of source fields, requires 6, got: " + sourceFields ); } @Override public void sourceConfInit( FlowProcess<? extends Properties> flowProcess, Tap<Properties, KafkaConsumerRecordIterator<String, String>, Producer<String, String>> tap, Properties conf ) { conf.setProperty( ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getCanonicalName() ); conf.setProperty( ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getCanonicalName() ); } @Override public void sinkConfInit( FlowProcess<? extends Properties> flowProcess, Tap<Properties, KafkaConsumerRecordIterator<String, String>, Producer<String, String>> tap, Properties conf ) { conf.setProperty( ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getCanonicalName() ); conf.setProperty( ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getCanonicalName() ); } @Override public void sourcePrepare( FlowProcess<? extends Properties> flowProcess, SourceCall<TextKafkaScheme.Context, KafkaConsumerRecordIterator<String, String>> sourceCall ) throws IOException { sourceCall.setContext( new Context( ( (KafkaTap) sourceCall.getTap() ).getTopics() ) ); } @Override public void sinkPrepare( FlowProcess<? extends Properties> flowProcess, SinkCall<TextKafkaScheme.Context, Producer<String, String>> sinkCall ) throws IOException { sinkCall.setContext( new Context( ( (KafkaTap) sinkCall.getTap() ).getTopics() ) ); } @Override public boolean source( FlowProcess<? extends Properties> flowProcess, SourceCall<TextKafkaScheme.Context, KafkaConsumerRecordIterator<String, String>> sourceCall ) throws IOException { Iterator<ConsumerRecord<String, String>> input = sourceCall.getInput(); if( !input.hasNext() ) return false; ConsumerRecord<String, String> record = input.next(); TupleEntry incomingEntry = sourceCall.getIncomingEntry(); incomingEntry.setObject( 0, record.topic() ); incomingEntry.setObject( 1, record.partition() ); incomingEntry.setObject( 2, record.offset() ); incomingEntry.setObject( 3, record.key() ); incomingEntry.setObject( 4, record.value() ); incomingEntry.setObject( 5, record.timestamp() ); incomingEntry.setObject( 6, record.timestampType() ); return true; } @Override public void sink( FlowProcess<? extends Properties> flowProcess, SinkCall<TextKafkaScheme.Context, Producer<String, String>> sinkCall ) throws IOException { String key = sinkCall.getOutgoingEntry().getString( 0 ); String value = sinkCall.getOutgoingEntry().getString( 1 ); for( String topic : sinkCall.getContext().topics ) sinkCall.getOutput().send( new ProducerRecord<>( topic, key, value ) ); } }
public class Hfs extends Tap<Configuration, RecordReader, OutputCollector> implements FileType<Configuration>, TapWith<Configuration, RecordReader, OutputCollector> { private static final Logger LOG = LoggerFactory.getLogger( Hfs.class ); protected String stringPath; transient URI uriScheme; transient Path path; private transient FileStatus[] statuses; private transient String cachedPath = null; private static final PathFilter HIDDEN_FILES_FILTER = path -> { String name = path.getName(); if( name.isEmpty() ) return true; char first = name.charAt( 0 ); return first != '_' && first != '.'; }; protected static String getLocalModeScheme( Configuration conf, String defaultValue ) { return conf.get( HfsProps.LOCAL_MODE_SCHEME, defaultValue ); } protected static boolean getUseCombinedInput( Configuration conf ) { boolean combineEnabled = conf.getBoolean( "cascading.hadoop.hfs.combine.files", false ); if( conf.get( FlowRuntimeProps.COMBINE_SPLITS ) == null && !combineEnabled ) return false; if( !combineEnabled ) combineEnabled = conf.getBoolean( FlowRuntimeProps.COMBINE_SPLITS, false ); String platform = conf.get( "cascading.flow.platform", "" ); if( platform.equals( "hadoop" ) || platform.equals( "hadoop3-mr1" ) ) return combineEnabled; if( conf.get( FlowRuntimeProps.COMBINE_SPLITS ) != null ) return false; if( combineEnabled && !Boolean.getBoolean( "cascading.hadoop.hfs.combine.files.warned" ) ) { LOG.warn( "'cascading.hadoop.hfs.combine.files' has been set to true, but is unsupported by this platform: {}, will be ignored to prevent failures", platform ); System.setProperty( "cascading.hadoop.hfs.combine.files.warned", "true" ); } return false; } protected static boolean getCombinedInputSafeMode( Configuration conf ) { return conf.getBoolean( "cascading.hadoop.hfs.combine.safemode", true ); } protected Hfs() { } @ConstructorProperties({"scheme"}) protected Hfs( Scheme<Configuration, RecordReader, OutputCollector, ?, ?> scheme ) { super( scheme ); } @ConstructorProperties({"scheme", "stringPath"}) public Hfs( Scheme<Configuration, RecordReader, OutputCollector, ?, ?> scheme, String stringPath ) { super( scheme ); setStringPath( stringPath ); } @ConstructorProperties({"scheme", "stringPath", "sinkMode"}) public Hfs( Scheme<Configuration, RecordReader, OutputCollector, ?, ?> scheme, String stringPath, SinkMode sinkMode ) { super( scheme, sinkMode ); setStringPath( stringPath ); } @ConstructorProperties({"scheme", "path"}) public Hfs( Scheme<Configuration, RecordReader, OutputCollector, ?, ?> scheme, Path path ) { super( scheme ); setStringPath( path.toString() ); } @ConstructorProperties({"scheme", "path", "sinkMode"}) public Hfs( Scheme<Configuration, RecordReader, OutputCollector, ?, ?> scheme, Path path, SinkMode sinkMode ) { super( scheme, sinkMode ); setStringPath( path.toString() ); } @Override public TapWith<Configuration, RecordReader, OutputCollector> withChildIdentifier( String identifier ) { Path path = new Path( identifier ); if( !path.toString().startsWith( getPath().toString() ) ) path = new Path( getPath(), path ); return create( getScheme(), path, getSinkMode() ); } @Override public TapWith<Configuration, RecordReader, OutputCollector> withScheme( Scheme<Configuration, RecordReader, OutputCollector, ?, ?> scheme ) { return create( scheme, getPath(), getSinkMode() ); } @Override public TapWith<Configuration, RecordReader, OutputCollector> withSinkMode( SinkMode sinkMode ) { return create( getScheme(), getPath(), sinkMode ); } protected TapWith<Configuration, RecordReader, OutputCollector> create( Scheme<Configuration, RecordReader, OutputCollector, ?, ?> scheme, Path path, SinkMode sinkMode ) { try { return Util.newInstance( getClass(), new Object[]{scheme, path, sinkMode} ); } catch( CascadingException exception ) { throw new TapException( "unable to create a new instance of: " + getClass().getName(), exception ); } } protected void setStringPath( String stringPath ) { this.stringPath = Util.normalizeUrl( stringPath ); } protected void setUriScheme( URI uriScheme ) { this.uriScheme = uriScheme; } public URI getURIScheme( Configuration jobConf ) { if( uriScheme != null ) return uriScheme; uriScheme = makeURIScheme( jobConf ); return uriScheme; } protected URI makeURIScheme( Configuration configuration ) { try { URI uriScheme; LOG.debug( "handling path: {}", stringPath ); URI uri = new Path( stringPath ).toUri(); String schemeString = uri.getScheme(); String authority = uri.getAuthority(); LOG.debug( "found scheme: {}, authority: {}", schemeString, authority ); if( schemeString != null && authority != null ) uriScheme = new URI( schemeString + ": else if( schemeString != null ) uriScheme = new URI( schemeString + ": else uriScheme = getDefaultFileSystemURIScheme( configuration ); LOG.debug( "using uri scheme: {}", uriScheme ); return uriScheme; } catch( URISyntaxException exception ) { throw new TapException( "could not determine scheme from path: " + getPath(), exception ); } } public URI getDefaultFileSystemURIScheme( Configuration configuration ) { return getDefaultFileSystem( configuration ).getUri(); } protected FileSystem getDefaultFileSystem( Configuration configuration ) { try { return FileSystem.get( configuration ); } catch( IOException exception ) { throw new TapException( "unable to get handle to underlying filesystem", exception ); } } protected FileSystem getFileSystem( Configuration configuration ) { URI scheme = getURIScheme( configuration ); try { return FileSystem.get( scheme, configuration ); } catch( IOException exception ) { throw new TapException( "unable to get handle to get filesystem for: " + scheme.getScheme(), exception ); } } @Override public String getIdentifier() { if( cachedPath == null ) cachedPath = getPath().toString(); return cachedPath; } public Path getPath() { if( path != null ) return path; if( stringPath == null ) throw new IllegalStateException( "path not initialized" ); path = new Path( stringPath ); return path; } @Override public String getFullIdentifier( Configuration conf ) { return getPath().makeQualified( getFileSystem( conf ) ).toString(); } @Override public void sourceConfInit( FlowProcess<? extends Configuration> process, Configuration conf ) { String fullIdentifier = getFullIdentifier( conf ); applySourceConfInitIdentifiers( process, conf, fullIdentifier ); verifyNoDuplicates( conf ); } protected static void verifyNoDuplicates( Configuration conf ) { Path[] inputPaths = getDeclaredInputPaths( conf ); Set<Path> paths = new HashSet<Path>( (int) ( inputPaths.length / .75f ) ); for( Path inputPath : inputPaths ) { if( !paths.add( inputPath ) ) throw new TapException( "may not add duplicate paths, found: " + inputPath ); } } protected static Path[] getDeclaredInputPaths( Configuration conf ) { return FileInputFormat.getInputPaths( HadoopUtil.asJobConfInstance( conf ) ); } protected void applySourceConfInitIdentifiers( FlowProcess<? extends Configuration> process, Configuration conf, final String... fullIdentifiers ) { sourceConfInitAddInputPaths( conf, new LazyIterable<String, Path>( fullIdentifiers ) { @Override protected Path convert( String next ) { return new Path( next ); } } ); sourceConfInitComplete( process, conf ); } protected void sourceConfInitAddInputPaths( Configuration conf, Iterable<Path> qualifiedPaths ) { HadoopUtil.addInputPaths( conf, qualifiedPaths ); for( Path qualifiedPath : qualifiedPaths ) { boolean stop = !makeLocal( conf, qualifiedPath, "forcing job to stand-alone mode, via source: " ); if( stop ) break; } } @Deprecated protected void sourceConfInitAddInputPath( Configuration conf, Path qualifiedPath ) { HadoopUtil.addInputPath( conf, qualifiedPath ); makeLocal( conf, qualifiedPath, "forcing job to stand-alone mode, via source: " ); } protected void sourceConfInitComplete( FlowProcess<? extends Configuration> process, Configuration conf ) { super.sourceConfInit( process, conf ); TupleSerialization.setSerializations( conf ); handleCombineFileInputFormat( conf ); } private void handleCombineFileInputFormat( Configuration conf ) { if( !getUseCombinedInput( conf ) ) return; String individualInputFormat = conf.get( "mapred.input.format.class" ); if( individualInputFormat == null ) throw new TapException( "input format is missing from the underlying scheme" ); if( individualInputFormat.equals( CombinedInputFormat.class.getName() ) && conf.get( CombineFileRecordReaderWrapper.INDIVIDUAL_INPUT_FORMAT ) == null ) throw new TapException( "the input format class is already the combined input format but the underlying input format is missing" ); boolean safeMode = getCombinedInputSafeMode( conf ); if( !FileInputFormat.class.isAssignableFrom( conf.getClass( "mapred.input.format.class", null ) ) ) { if( safeMode ) throw new TapException( "input format must be of type org.apache.hadoop.mapred.FileInputFormat, got: " + individualInputFormat ); else LOG.warn( "not combining input splits with CombineFileInputFormat, {} is not of type org.apache.hadoop.mapred.FileInputFormat.", individualInputFormat ); } else { conf.set( CombineFileRecordReaderWrapper.INDIVIDUAL_INPUT_FORMAT, individualInputFormat ); conf.setClass( "mapred.input.format.class", CombinedInputFormat.class, InputFormat.class ); } } @Override public void sinkConfInit( FlowProcess<? extends Configuration> process, Configuration conf ) { Path qualifiedPath = new Path( getFullIdentifier( conf ) ); HadoopUtil.setOutputPath( conf, qualifiedPath ); super.sinkConfInit( process, conf ); makeLocal( conf, qualifiedPath, "forcing job to stand-alone mode, via sink: " ); TupleSerialization.setSerializations( conf ); } private boolean makeLocal( Configuration conf, Path qualifiedPath, String infoMessage ) { if( HadoopUtil.isInflow( conf ) ) return false; String scheme = getLocalModeScheme( conf, "file" ); if( !HadoopUtil.isLocal( conf ) && qualifiedPath.toUri().getScheme().equalsIgnoreCase( scheme ) ) { if( LOG.isInfoEnabled() ) LOG.info( infoMessage + toString() ); HadoopUtil.setLocal( conf ); return false; } return true; } @Override public TupleEntryIterator openForRead( FlowProcess<? extends Configuration> flowProcess, RecordReader input ) throws IOException { return new HadoopTupleEntrySchemeIterator( flowProcess, this, input ); } @Override public TupleEntryCollector openForWrite( FlowProcess<? extends Configuration> flowProcess, OutputCollector output ) throws IOException { resetFileStatuses(); return new HadoopTupleEntrySchemeCollector( flowProcess, this, output ); } @Override public boolean createResource( Configuration conf ) throws IOException { if( LOG.isDebugEnabled() ) LOG.debug( "making dirs: {}", getFullIdentifier( conf ) ); return getFileSystem( conf ).mkdirs( getPath() ); } @Override public boolean deleteResource( Configuration conf ) throws IOException { String fullIdentifier = getFullIdentifier( conf ); return deleteFullIdentifier( conf, fullIdentifier ); } private boolean deleteFullIdentifier( Configuration conf, String fullIdentifier ) throws IOException { if( LOG.isDebugEnabled() ) LOG.debug( "deleting: {}", fullIdentifier ); resetFileStatuses(); Path fullPath = new Path( fullIdentifier ); if( fullPath.depth() == 0 ) return true; FileSystem fileSystem = getFileSystem( conf ); try { return fileSystem.delete( fullPath, true ); } catch( NullPointerException exception ) { if( !( fileSystem.getClass().getSimpleName().equals( "NativeS3FileSystem" ) ) ) throw exception; } return true; } public boolean deleteChildResource( FlowProcess<? extends Configuration> flowProcess, String childIdentifier ) throws IOException { return deleteChildResource( flowProcess.getConfig(), childIdentifier ); } public boolean deleteChildResource( Configuration conf, String childIdentifier ) throws IOException { resetFileStatuses(); Path childPath = new Path( childIdentifier ).makeQualified( getFileSystem( conf ) ); if( !childPath.toString().startsWith( getFullIdentifier( conf ) ) ) return false; return deleteFullIdentifier( conf, childPath.toString() ); } @Override public boolean resourceExists( Configuration conf ) throws IOException { FileStatus[] fileStatuses = getFileSystem( conf ).globStatus( getPath() ); return fileStatuses != null && fileStatuses.length > 0; } @Override public boolean isDirectory( FlowProcess<? extends Configuration> flowProcess ) throws IOException { return isDirectory( flowProcess.getConfig() ); } @Override public boolean isDirectory( Configuration conf ) throws IOException { if( !resourceExists( conf ) ) return false; return getFileSystem( conf ).getFileStatus( getPath() ).isDir(); } @Override public long getSize( FlowProcess<? extends Configuration> flowProcess ) throws IOException { return getSize( flowProcess.getConfig() ); } @Override public long getSize( Configuration conf ) throws IOException { if( !resourceExists( conf ) ) return 0; FileStatus fileStatus = getFileStatus( conf ); if( fileStatus.isDir() ) return 0; return getFileSystem( conf ).getFileStatus( getPath() ).getLen(); } public long getBlockSize( FlowProcess<? extends Configuration> flowProcess ) throws IOException { return getBlockSize( flowProcess.getConfig() ); } public long getBlockSize( Configuration conf ) throws IOException { if( !resourceExists( conf ) ) return 0; FileStatus fileStatus = getFileStatus( conf ); if( fileStatus.isDir() ) return 0; return fileStatus.getBlockSize(); } public int getReplication( FlowProcess<? extends Configuration> flowProcess ) throws IOException { return getReplication( flowProcess.getConfig() ); } public int getReplication( Configuration conf ) throws IOException { if( !resourceExists( conf ) ) return 0; FileStatus fileStatus = getFileStatus( conf ); if( fileStatus.isDir() ) return 0; return fileStatus.getReplication(); } @Override public String[] getChildIdentifiers( FlowProcess<? extends Configuration> flowProcess ) throws IOException { return getChildIdentifiers( flowProcess.getConfig(), 1, false ); } @Override public String[] getChildIdentifiers( Configuration conf ) throws IOException { return getChildIdentifiers( conf, 1, false ); } @Override public String[] getChildIdentifiers( FlowProcess<? extends Configuration> flowProcess, int depth, boolean fullyQualified ) throws IOException { return getChildIdentifiers( flowProcess.getConfig(), depth, fullyQualified ); } @Override public String[] getChildIdentifiers( Configuration conf, int depth, boolean fullyQualified ) throws IOException { if( !resourceExists( conf ) ) return new String[ 0 ]; if( depth == 0 && !fullyQualified ) return new String[]{getIdentifier()}; String rootIdentifier = getFullIdentifier( conf ); int trim = fullyQualified ? 0 : rootIdentifier.length() + 1; Path[] inputPaths = getDeclaredInputPaths( conf ); if( inputPaths.length == 0 ) inputPaths = new Path[]{new Path( rootIdentifier )}; Set<String> results = new LinkedHashSet<>(); for( Path identifier : inputPaths ) getChildPaths( conf, results, trim, identifier, depth ); return results.toArray( new String[ 0 ] ); } private void getChildPaths( Configuration conf, Set<String> results, int trim, Path path, int depth ) throws IOException { if( depth == 0 ) { String substring = path.toString().substring( trim ); String identifier = getIdentifier(); if( identifier == null || identifier.isEmpty() ) results.add( new Path( substring ).toString() ); else results.add( new Path( identifier, substring ).toString() ); return; } FileStatus[] statuses = getFileSystem( conf ).listStatus( path, HIDDEN_FILES_FILTER ); if( statuses == null ) return; for( FileStatus fileStatus : statuses ) getChildPaths( conf, results, trim, fileStatus.getPath(), depth - 1 ); } @Override public long getModifiedTime( Configuration conf ) throws IOException { if( !resourceExists( conf ) ) return 0; FileStatus fileStatus = getFileStatus( conf ); if( !fileStatus.isDir() ) return fileStatus.getModificationTime(); makeStatuses( conf ); if( statuses == null || statuses.length == 0 ) return 0; long date = 0; for( FileStatus status : statuses ) { if( !status.isDir() ) date = Math.max( date, status.getModificationTime() ); } return date; } public FileStatus getFileStatus( Configuration conf ) throws IOException { return getFileSystem( conf ).getFileStatus( getPath() ); } public static Path getTempPath( Configuration conf ) { String tempDir = conf.get( HfsProps.TEMPORARY_DIRECTORY ); if( tempDir == null ) tempDir = conf.get( "hadoop.tmp.dir" ); return new Path( tempDir ); } protected String makeTemporaryPathDirString( String name ) { name = name.replaceAll( "^[_\\W\\s]+", "" ); if( name.isEmpty() ) name = "temp-path"; return name.replaceAll( "[\\W\\s]+", "_" ) + Util.createUniqueID(); } private void makeStatuses( Configuration conf ) throws IOException { if( statuses != null ) return; statuses = getFileSystem( conf ).listStatus( getPath() ); } public void resetFileStatuses() { statuses = null; } static class CombinedInputFormat extends CombineFileInputFormat implements Configurable { private Configuration conf; public RecordReader getRecordReader( InputSplit split, JobConf job, Reporter reporter ) throws IOException { return new CombineFileRecordReader( job, (CombineFileSplit) split, reporter, CombineFileRecordReaderWrapper.class ); } @Override public void setConf( Configuration conf ) { this.conf = conf; setMaxSplitSize( conf.getLong( "cascading.hadoop.hfs.combine.max.size", 0 ) ); } @Override public Configuration getConf() { return conf; } } }
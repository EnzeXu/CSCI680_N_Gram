public class HadoopNodeStats extends BaseCachedNodeStats<Configuration, FlowNodeStats, Map<String, Map<String, Long>>> { private Map<TaskID, String> sliceIDCache = new HashMap<TaskID, String>( 4999 ); private HadoopStepStats parentStepStats; private HadoopSliceStats.Kind kind; protected HadoopNodeStats( final HadoopStepStats parentStepStats, Configuration configuration, HadoopSliceStats.Kind kind, FlowNode flowNode, ClientState clientState ) { super( flowNode, clientState ); this.parentStepStats = parentStepStats; this.kind = kind; this.counterCache = new HadoopNodeCounterCache( this, configuration ); } @Override public String getKind() { if( kind == null ) return null; return kind.name(); } private Status getParentStatus() { return parentStepStats.getStatus(); } private RunningJob getJobStatusClient() { return parentStepStats.getJobStatusClient(); } private TaskReport[] retrieveTaskReports( HadoopSliceStats.Kind kind ) throws IOException, InterruptedException { Job job = HadoopStepStats.getJob( getJobStatusClient() ); if( job == null ) return new TaskReport[ 0 ]; switch( kind ) { case MAPPER: return job.getTaskReports( TaskType.MAP ); case REDUCER: return job.getTaskReports( TaskType.REDUCE ); case SETUP: return job.getTaskReports( TaskType.JOB_SETUP ); case CLEANUP: return job.getTaskReports( TaskType.JOB_CLEANUP ); default: return new TaskReport[ 0 ]; } } @Override protected boolean captureChildDetailInternal() { if( allChildrenFinished ) return true; Job job = HadoopStepStats.getJob( getJobStatusClient() ); if( job == null ) return false; try { TaskReport[] taskReports = retrieveTaskReports( kind ); if( taskReports.length == 0 ) return false; addTaskStats( taskReports, false ); return true; } catch( IOException exception ) { logWarn( "unable to retrieve slice stats via task reports", exception ); } catch( InterruptedException exception ) { logWarn( "retrieving task reports timed out, consider increasing timeout delay in CounterCache via: '{}', message: {}", CounterCache.COUNTER_TIMEOUT_PROPERTY, exception.getMessage() ); } return false; } protected void addTaskStats( TaskReport[] taskReports, boolean skipLast ) { logInfo( "retrieved task reports: {}", taskReports.length ); long lastFetch = System.currentTimeMillis(); boolean fetchedAreFinished = true; synchronized( sliceStatsMap ) { int added = 0; int updated = 0; for( int i = 0; i < taskReports.length - ( skipLast ? 1 : 0 ); i++ ) { TaskReport taskReport = taskReports[ i ]; if( taskReport == null ) { logWarn( "found empty task report" ); continue; } String id = getSliceIDFor( taskReport.getTaskID() ); HadoopSliceStats sliceStats = (HadoopSliceStats) sliceStatsMap.get( id ); if( sliceStats != null ) { sliceStats.update( getParentStatus(), kind, taskReport, lastFetch ); updated++; } else { sliceStats = new HadoopSliceStats( id, getParentStatus(), kind, taskReport, lastFetch ); sliceStatsMap.put( id, sliceStats ); added++; } if( !sliceStats.getStatus().isFinished() ) fetchedAreFinished = false; } int total = sliceStatsMap.size(); String duration = formatDurationFromMillis( System.currentTimeMillis() - lastFetch ); logInfo( "added {}, updated: {} slices, with duration: {}, total fetched: {}", added, updated, duration, total ); } allChildrenFinished = taskReports.length != 0 && fetchedAreFinished; } protected void addAttempt( TaskCompletionEvent event ) { String sliceID = sliceIDCache.get( event.getTaskAttemptId().getTaskID() ); if( sliceID == null ) return; FlowSliceStats stats; synchronized( sliceStatsMap ) { stats = sliceStatsMap.get( sliceID ); } if( stats == null ) return; ( (HadoopSliceStats) stats ).addAttempt( event ); } private String getSliceIDFor( TaskID taskID ) { String id = sliceIDCache.get( taskID ); if( id == null ) { id = Util.createUniqueID(); sliceIDCache.put( taskID, id ); } return id; } }
public class HadoopFlowStep extends BaseFlowStep<JobConf> { protected HadoopFlowStep() { } protected HadoopFlowStep( String name, int ordinal ) { super( name, ordinal ); } public HadoopFlowStep( ElementGraph elementGraph, FlowNodeGraph flowNodeGraph ) { super( elementGraph, flowNodeGraph ); } @Override public Map<Object, Object> getConfigAsProperties() { return HadoopUtil.createProperties( getConfig() ); } public JobConf createInitializedConfig( FlowProcess<JobConf> flowProcess, JobConf parentConfig ) { JobConf conf = parentConfig == null ? new JobConf() : HadoopUtil.copyJobConf( parentConfig ); conf.setBoolean( "mapred.used.genericoptionsparser", true ); conf.setJobName( getStepDisplayName( conf.getInt( "cascading.display.id.truncate", Util.ID_LENGTH ) ) ); conf.setOutputKeyClass( KeyTuple.class ); conf.setOutputValueClass( ValueTuple.class ); conf.setMapRunnerClass( FlowMapper.class ); conf.setReducerClass( FlowReducer.class ); Set<String> serializations = getFieldDeclaredSerializations( Serialization.class ); TupleSerialization.setSerializations( conf, serializations ); initFromSources( flowProcess, conf ); initFromSink( flowProcess, conf ); initFromTraps( flowProcess, conf ); initFromStepConfigDef( conf ); int numSinkParts = getSink().getScheme().getNumSinkParts(); if( numSinkParts != 0 ) { if( getGroup() != null ) conf.setNumReduceTasks( numSinkParts ); else conf.setNumMapTasks( numSinkParts ); } else if( getGroup() != null ) { int gatherPartitions = conf.getNumReduceTasks(); if( gatherPartitions == 0 ) gatherPartitions = conf.getInt( FlowRuntimeProps.GATHER_PARTITIONS, 0 ); if( gatherPartitions == 0 ) throw new FlowException( getName(), "a default number of gather partitions must be set, see FlowRuntimeProps" ); conf.setNumReduceTasks( gatherPartitions ); } conf.setOutputKeyComparatorClass( TupleComparator.class ); ProcessEdge processEdge = Util.getFirst( getFlowNodeGraph().edgeSet() ); if( getGroup() == null ) { conf.setNumReduceTasks( 0 ); } else { conf.setMapOutputKeyClass( KeyTuple.class ); conf.setMapOutputValueClass( ValueTuple.class ); conf.setPartitionerClass( GroupingPartitioner.class ); if( getGroup().isSortReversed() ) conf.setOutputKeyComparatorClass( ReverseTupleComparator.class ); Integer ordinal = (Integer) Util.getFirst( processEdge.getSinkExpectedOrdinals() ); addComparators( conf, "cascading.group.comparator", getGroup().getKeySelectors(), (Fields) processEdge.getResolvedKeyFields().get( ordinal ) ); if( getGroup().isGroupBy() ) addComparators( conf, "cascading.sort.comparator", getGroup().getSortingSelectors(), (Fields) processEdge.getResolvedSortFields().get( ordinal ) ); if( !getGroup().isGroupBy() ) { conf.setPartitionerClass( CoGroupingPartitioner.class ); conf.setMapOutputKeyClass( KeyIndexTuple.class ); conf.setMapOutputValueClass( ValueIndexTuple.class ); conf.setOutputKeyComparatorClass( IndexTupleCoGroupingComparator.class ); conf.setOutputValueGroupingComparator( CoGroupingComparator.class ); } if( getGroup().isSorted() ) { conf.setPartitionerClass( GroupingSortingPartitioner.class ); conf.setMapOutputKeyClass( TuplePair.class ); if( getGroup().isSortReversed() ) conf.setOutputKeyComparatorClass( ReverseGroupingSortingComparator.class ); else conf.setOutputKeyComparatorClass( GroupingSortingComparator.class ); conf.setOutputValueGroupingComparator( GroupingComparator.class ); } } if( processEdge != null && ifCoGroupAndKeysHaveCommonTypes( this, processEdge.getFlowElement(), processEdge.getResolvedKeyFields() ) ) { conf.set( "cascading.node.ordinals", Util.join( processEdge.getSinkExpectedOrdinals(), "," ) ); addFields( conf, "cascading.node.key.fields", processEdge.getResolvedKeyFields() ); addFields( conf, "cascading.node.sort.fields", processEdge.getResolvedSortFields() ); addFields( conf, "cascading.node.value.fields", processEdge.getResolvedValueFields() ); } String versionString = Version.getRelease(); if( versionString != null ) conf.set( "cascading.version", versionString ); conf.set( CASCADING_FLOW_STEP_ID, getID() ); conf.set( "cascading.flow.step.num", Integer.toString( getOrdinal() ) ); HadoopUtil.setIsInflow( conf ); Iterator<FlowNode> iterator = getFlowNodeGraph().getTopologicalIterator(); FlowNode mapperNode = iterator.next(); FlowNode reducerNode = iterator.hasNext() ? iterator.next() : null; if( reducerNode != null ) reducerNode.addProcessAnnotation( FlowRuntimeProps.GATHER_PARTITIONS, Integer.toString( conf.getNumReduceTasks() ) ); String mapState = pack( mapperNode, conf ); String reduceState = pack( reducerNode, conf ); int maxSize = Short.MAX_VALUE; int length = mapState.length() + reduceState.length(); if( isHadoopLocalMode( conf ) || length < maxSize ) { conf.set( "cascading.flow.step.node.map", mapState ); if( !Util.isEmpty( reduceState ) ) conf.set( "cascading.flow.step.node.reduce", reduceState ); } else { conf.set( "cascading.flow.step.node.map.path", HadoopMRUtil.writeStateToDistCache( conf, getID(), "map", mapState ) ); if( !Util.isEmpty( reduceState ) ) conf.set( "cascading.flow.step.node.reduce.path", HadoopMRUtil.writeStateToDistCache( conf, getID(), "reduce", reduceState ) ); } return conf; } private static boolean ifCoGroupAndKeysHaveCommonTypes( ProcessLogger processLogger, FlowElement flowElement, Map<Integer, Fields> resolvedKeyFields ) { if( !( flowElement instanceof CoGroup ) ) return true; if( resolvedKeyFields == null || resolvedKeyFields.size() < 2 ) return true; Iterator<Map.Entry<Integer, Fields>> iterator = resolvedKeyFields.entrySet().iterator(); Fields fields = iterator.next().getValue(); while( iterator.hasNext() ) { Fields next = iterator.next().getValue(); if( !Arrays.equals( fields.getTypesClasses(), next.getTypesClasses() ) ) { processLogger.logWarn( "unable to perform: {}, on mismatched join types and optimize serialization with type exclusion, fields: {} & {}", flowElement, fields, next ); return false; } } return true; } public boolean isHadoopLocalMode( JobConf conf ) { return HadoopUtil.isLocal( conf ); } protected FlowStepJob<JobConf> createFlowStepJob( ClientState clientState, FlowProcess<JobConf> flowProcess, JobConf initializedStepConfig ) { try { return new HadoopFlowStepJob( clientState, this, initializedStepConfig ); } catch( NoClassDefFoundError error ) { PlatformInfo platformInfo = HadoopUtil.getPlatformInfo( JobConf.class, "org/apache/hadoop", "Hadoop MR" ); String message = "unable to load platform specific class, please verify Hadoop cluster version: '%s', matches the Hadoop platform build dependency and associated FlowConnector, cascading-hadoop or cascading-hadoop3-mr1"; logError( String.format( message, platformInfo.toString() ), error ); throw error; } } public void clean( JobConf config ) { String stepStatePath = config.get( "cascading.flow.step.path" ); if( stepStatePath != null ) { try { HadoopUtil.removeStateFromDistCache( config, stepStatePath ); } catch( IOException exception ) { logWarn( "unable to remove step state file: " + stepStatePath, exception ); } } if( tempSink != null ) { try { tempSink.deleteResource( config ); } catch( Exception exception ) { logWarn( "unable to remove temporary file: " + tempSink, exception ); } } for( Tap sink : getSinkTaps() ) cleanIntermediateData( config, sink ); for( Tap tap : getTraps() ) cleanTapMetaData( config, tap ); } protected void cleanIntermediateData( JobConf config, Tap sink ) { if( sink.isTemporary() && ( getFlow().getFlowStats().isSuccessful() || getFlow().getRunID() == null ) ) { try { sink.deleteResource( config ); } catch( Exception exception ) { logWarn( "unable to remove temporary file: " + sink, exception ); } } else { cleanTapMetaData( config, sink ); } } private void cleanTapMetaData( JobConf jobConf, Tap tap ) { try { Hadoop18TapUtil.cleanupTapMetaData( jobConf, tap ); } catch( IOException exception ) { } } private void initFromTraps( FlowProcess<JobConf> flowProcess, JobConf conf, Map<String, Tap> traps ) { if( !traps.isEmpty() ) { JobConf trapConf = HadoopUtil.copyJobConf( conf ); for( Tap tap : traps.values() ) tap.sinkConfInit( flowProcess, trapConf ); } } protected void initFromSources( FlowProcess<JobConf> flowProcess, JobConf conf ) { Set<Tap> uniqueSources = getUniqueStreamedSources(); JobConf[] streamedJobs = new JobConf[ uniqueSources.size() ]; int i = 0; for( Tap tap : uniqueSources ) { if( tap.getIdentifier() == null ) throw new IllegalStateException( "tap may not have null identifier: " + tap.toString() ); streamedJobs[ i ] = flowProcess.copyConfig( conf ); streamedJobs[ i ].set( "cascading.step.source", Tap.id( tap ) ); tap.sourceConfInit( flowProcess, streamedJobs[ i ] ); i++; } Set<Tap> accumulatedSources = getAllAccumulatedSources(); for( Tap tap : accumulatedSources ) { JobConf accumulatedJob = flowProcess.copyConfig( conf ); tap.sourceConfInit( flowProcess, accumulatedJob ); Map<String, String> map = flowProcess.diffConfigIntoMap( conf, accumulatedJob ); conf.set( "cascading.node.accumulated.source.conf." + Tap.id( tap ), pack( map, conf ) ); try { if( DistributedCache.getCacheFiles( accumulatedJob ) != null ) DistributedCache.setCacheFiles( DistributedCache.getCacheFiles( accumulatedJob ), conf ); } catch( IOException exception ) { throw new CascadingException( exception ); } } MultiInputFormat.addInputFormat( conf, streamedJobs ); } private void initFromStepConfigDef( final JobConf conf ) { initConfFromStepConfigDef( new ConfigurationSetter( conf ) ); } private Set<Tap> getUniqueStreamedSources() { Set<Tap> allAccumulatedSources = getAllAccumulatedSources(); allAccumulatedSources.removeAll( getAllStreamedSources() ); HashSet<Tap> set = new HashSet<>( sources.keySet() ); set.removeAll( allAccumulatedSources ); return set; } protected void initFromSink( FlowProcess<JobConf> flowProcess, JobConf conf ) { if( getSink() != null ) getSink().sinkConfInit( flowProcess, conf ); Class<? extends OutputFormat> outputFormat = conf.getClass( "mapred.output.format.class", null, OutputFormat.class ); boolean isFileOutputFormat = false; if( outputFormat != null ) isFileOutputFormat = FileOutputFormat.class.isAssignableFrom( outputFormat ); Path outputPath = FileOutputFormat.getOutputPath( conf ); if( outputPath == null && ( isFileOutputFormat || outputFormat == null ) ) tempSink = new TempHfs( conf, "tmp:/" + new Path( getSink().getIdentifier() ).toUri().getPath(), true ); if( tempSink != null ) tempSink.sinkConfInit( flowProcess, conf ); } protected void initFromTraps( FlowProcess<JobConf> flowProcess, JobConf conf ) { initFromTraps( flowProcess, conf, getTrapMap() ); } }
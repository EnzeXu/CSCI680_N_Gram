public class HadoopMRUtil { private static final Logger LOG = LoggerFactory.getLogger( HadoopMRUtil.class ); public static String writeStateToDistCache( JobConf conf, String id, String kind, String stepState ) { if( Util.isEmpty( stepState ) ) return null; LOG.info( "writing step state to dist cache, too large for job conf, size: {}", stepState.length() ); String statePath = Hfs.getTempPath( conf ) + "/" + kind + "-state-" + id; Hfs temp = new Hfs( new TextLine(), statePath, SinkMode.REPLACE ); try { TupleEntryCollector writer = temp.openForWrite( new HadoopFlowProcess( conf ) ); writer.add( new Tuple( stepState ) ); writer.close(); } catch( IOException exception ) { throw new FlowException( "unable to write step state to Hadoop FS: " + temp.getIdentifier() ); } URI uri = new Path( statePath ).toUri(); DistributedCache.addCacheFile( uri, conf ); LOG.info( "using step state path: {}", uri ); return statePath; } public static String readStateFromDistCache( JobConf jobConf, String id, String kind ) throws IOException { Path[] files = DistributedCache.getLocalCacheFiles( jobConf ); Path stepStatePath = null; for( Path file : files ) { if( !file.toString().contains( kind + "-state-" + id ) ) continue; stepStatePath = file; break; } if( stepStatePath == null ) throw new FlowException( "unable to find step state from distributed cache" ); LOG.info( "reading step state from local path: {}", stepStatePath ); Hfs temp = new Lfs( new TextLine( new Fields( "line" ) ), stepStatePath.toString() ); TupleEntryIterator reader = null; try { reader = temp.openForRead( new HadoopFlowProcess( jobConf ) ); if( !reader.hasNext() ) throw new FlowException( "step state path is empty: " + temp.getIdentifier() ); return reader.next().getString( 0 ); } catch( IOException exception ) { throw new FlowException( "unable to find state path: " + temp.getIdentifier(), exception ); } finally { try { if( reader != null ) reader.close(); } catch( IOException exception ) { LOG.warn( "error closing state path reader", exception ); } } } public static Map<Path, Path> addToClassPath( Configuration config, List<String> classpath ) { if( classpath == null ) return null; Map<String, Path> localPaths = new HashMap<String, Path>(); Map<String, Path> remotePaths = new HashMap<String, Path>(); HadoopUtil.resolvePaths( config, classpath, null, null, localPaths, remotePaths ); try { LocalFileSystem localFS = HadoopUtil.getLocalFS( config ); for( String path : localPaths.keySet() ) { if( remotePaths.containsKey( path ) ) continue; Path artifact = localPaths.get( path ); DistributedCache.addFileToClassPath( artifact.makeQualified( localFS ), config ); } FileSystem defaultFS = HadoopUtil.getDefaultFS( config ); for( String path : remotePaths.keySet() ) { Path artifact = remotePaths.get( path ); DistributedCache.addFileToClassPath( artifact.makeQualified( defaultFS ), config ); } } catch( IOException exception ) { throw new FlowException( "unable to set distributed cache paths", exception ); } return HadoopUtil.getCommonPaths( localPaths, remotePaths ); } public static boolean hasReducer( JobConf jobConf ) { return jobConf.getReducerClass() != null; } }